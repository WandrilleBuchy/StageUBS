---
title: "Stage UBS"
author: "Wandrille Buchy"
format: html
editor: visual
---

# Filtre de Kalman et lissage RTS

## Introduction du modèle

L'objectif est d'implémenter le filtre de Kalman et le lissage RTS qui en découle. Pour cela, nous nous intéressons au *Hidden Markov Model* ou *HMM* représenté ci-bas avec les notations suivantes pour le modèle de dimension une :

![Modèle HMM](D:/Users/buchy/Documents/Stage/Schema_HHM.png){fig-align="center"}

-   $X_{\text{i}}$ est la loi de l'état du système a l'instant i du modèle. Cet état est caché et un objectif sera d'obtenir sa loi conditionnelle sachant un ou plusieurs $Y_{\text{i}}$ qui sont les mesures prises des états. Nous nous intéresserons à la loi de transition de densité $p(x_{\text{i}} \mid x_{\text{i-1}})$ qui est représentée sur la figure par les flèches continues passant de $x_{\text{i-1}}$ à $x_{\text{i}}$. Nous supposons qu'elle est de la forme

    #### $$
    X_{\text{i}} \mid X_{\text{i-1}}\sim \mathcal{N}(\alpha X_{\text{i-1}} + \beta, \sigma_{\text{trans}}^2)
    $$

    Avec $\alpha$ et $\beta$ des paramètres du modèle linéaire gaussien et $\sigma_{\text{trans}}^2$ la variance de la transition entre les états du système. On comprend aussi qu'il n'y a pas d'indépendance entre les $X_{\text{i}}$. $X_{\text{0}}$ initialise le modèle comme suivant :

    $$
    X_{\text{0}}\sim \mathcal{N}(\mu, \sigma^2)
    $$

    avec $\mu$ la moyenne des valeurs prises par $X_{\text{0}}$ et $\sigma^2$ pour sa variance.

-   Loi conditionnelle de $Y_{\text{i}}$ sachant $X_{\text{i}}$

    $$
    Y_{\text{i}}\mid X_{\text{i}} \sim \mathcal{N}(aX_{\text{i}} + b, \sigma_{\text{err}}^2)
    $$

    avec $a$ et $b$ des paramètres du modèle linéaire gaussien et $\sigma_{\text{err}}^2$ la variance du bruit subit. Cette loi est représentée par les flèches pointillées sur la figure.

-   $Y_{\text{i}}$ est la loi des mesures du modèle que nous avons; il s'agit de données récoltées de $X_{\text{i}}$ avec un bruit qui sont données par la loi

    $$
    Y_{\text{i}}\sim \mathcal{N}(a\mu + b , a^2\sigma^2 + \sigma_{\text{err}}^2)
    $$

    cette loi à pu être récupérée grâce aux deux précédentes.

Ceci nous permet d'étudier le modèle pour une dimension.

Nous allons maintenant repréciser les loi pour de multiples dimensions.

-   Premièrement, $X_{\text{i}}$ à valeur dans $\mathbb{R}^d$ admet une loi de transition sous la forme

    $$
    X_{\text{i}} \mid X_{\text{i-1}}\sim \mathcal{N}(F X_{\text{i-1}},Q)
    $$

    avec $F \in \mathbb{R}^{d \times d}$ une matrice arbitraire paramètre du modèle linéaire gaussien et $Q \in \mathbb{R}^{d \times d}$ une matrice de covariance et est donc dans notre cas symétrique définie positive. De même que pour une seule dimension, nous initialisons $X_{\text{0}}$ avec une gaussienne

    $$
    X_{\text{0}}\sim \mathcal{N}(\mu, V_{\text{0}})
    $$

    pour un $\mu \in \mathbb{R}^d$ l'espérance de la variable aléatoire et $V_{\text{0}}$ la matrice de covariance symétrique définie positive initiale.

-   Ensuite vient la loi conditionnelle de $Y_{\text{i}}$ sachant $X_{\text{i}}$ à valeur dans $\mathbb{R}^D$

    $$
    Y_{\text{i}}\mid X_{\text{i}} \sim \mathcal{N}(HX_{\text{i}},R)
    $$

    avec $H \in \mathbb{R}^{D \times d}$ la matrice paramètre du modèle linéaire gaussien et $R \in \mathbb{R}^{D \times D}$ la matrice de covariance comme les autres symétrique et définie postive.

-   Enfin $Y_{\text{i}}$ à valeur dans $\mathbb{R}^D$ définie comme telle

    $$
    Y_{\text{i}}\sim \mathcal{N}(A\mu, AV_{\text{0}}A^T +R)
    $$

Une fois que nous avons posé le cadre, nous étudions le modèle.

## Création des données

```{r initialisation}
#| echo: false

library(ggplot2)
library(plot3D)
library(dplyr)
library(tibble)
library(tidyr)
library(tidyverse)
library(RVCompare)

```

Nous créons des fonctions qui nous serons utiles pour la suite. Une fonction *get_mat_sqrt* qui donne la racine carrée d'une matrice, *get_simulation* qui génère des donnés selon la loi de distribution précisée plus tôt et enfin la fonction *get_data* qui nous permet d'obtenir le nombre de données voulu.

```{r création de la fonction de génération des données pour tout choix de dimensions}

#Fonction racine carrée de matrice
get_mat_sqrt <- function(A){
  if(is.null(dim(A))){
    A <- matrix(A)
  }
  y = eigen(A)
  MCV <- matrix(y$vectors,byrow=F,ncol=dim(A)[1]) #matrice orthogonale de vecteurs propres
  MCVinv <- t(MCV) #transposée de matrice de vecteur propre
  Diag <- diag(y$values, nrow = dim(A)[1]) #matrice diagonale des valeurs propres
  DiagCalc <- sqrt(Diag) # matrice diagonale racine carrée des valeurs propres
  MCV %*% DiagCalc
}


#Fonction génération d'une donnée de n dimensions
get_simulation <- function(n_sim, pars){
  x_white_noise <- matrix(rnorm(n_sim * pars$dim_x),
                          nrow = pars$dim_x,
                          ncol = n_sim)
  x <- pars$matF %*% pars$m_x + get_mat_sqrt(pars$S_x) %*% x_white_noise
  y_white_noise <- matrix(rnorm(n_sim * pars$dim_y),
                          nrow = pars$dim_y,
                          ncol = n_sim)
  y <- pars$A %*% x + pars$b +  get_mat_sqrt(pars$S_y) %*% y_white_noise
  return(list(x = t(x), y = t(y)))
}


#Fonction de création des N données
get_data <- function(n_sim, pars_X0, pars){
  
  Data <- get_simulation(1, pars_X0) #Création de la première donnée
  X_loop <- Data$x #Récupération de la donnée pour l'utiliser en argument de la suivante
  pars$m_x <- t(X_loop) #Ajout dans les paramètre de la donnée récupérée

  
  for (i in 1:(n_sim-1)){ #Boucle de la création des données suivantes
    
    append <- get_simulation(1, pars) #Création de la donnée en fonction de la précédente
    X_loop <- append$x #Récupération de la donnée pour l'utiliser en argument de la suivante
    pars$m_x <- as.matrix(t(X_loop)) #Ajout dans les paramètre de la donnée récupérée
    Data <- bind_rows(Data,append) #Enregistrement des valeurs de X et Y dans le Data
  }
  return(Data)
}


```

Ceci fait, nous utilisons les paramètres suivants pour créer les données.

```{r création de toutes les données}


pars_X0_1d <- list(dim_x = 1,
             dim_y = 1,
             m_x = 70,
             S_x = 2,
             A = 2,
             b = 0,
             matF = 1,
             S_y = 0.1)
pars_1d <- list(dim_x = 1, #pas de valeur moyenne 
             dim_y = 1,
             S_x = 2,
             A = 2,
             b = 0,
             matF = 0.995,
             S_y = 0.1)


pars_X0_2d <- list(dim_x = 2,
                dim_y = 2,
                m_x = rep(100, 2),
                S_x = diag(1.1, 2),
                A = matrix(0:3, nrow = 2, ncol = 2),
                b = rep(0, 2),
                matF = diag(1, 2), 
                S_y = diag(0.2, 2))
pars_2d <- list(dim_x = 2, #pas de valeur moyenne 
                dim_y = 2,
                S_x = diag(1.2, 2),
                A = matrix(0:3, nrow = 2, ncol = 2),
                b = rep(0, 2),
                matF = diag(0.995,2), 
                S_y = diag(0.2, 2))


n_obs <- 2000
data_1d <- get_data(n_obs, pars_X0_1d, pars_1d) #Données 1d
data_2d <- get_data(n_obs, pars_X0_2d, pars_2d) #Données 2d

```

## Représentation des données

Nous représentons les données en première dimension avec un histogramme.

```{r représentation des données pour une seule dimension en histogramme}
#| echo: false

data_histo <- rep(1,n_obs)
for (i in 1:n_obs){
  data_histo[i] <- get_simulation(1, pars_X0_1d)$y[,1]
}

#fonction gaussienne de la répartition des tirages Y
Y_densite <- function(x, pars){
  return(dnorm(x, mean = pars$A * pars$m_x + pars$b , sd = sqrt(pars$A^2 * pars$S_x + pars$S_y)))
}


#représentation graphique de l'échantillon créé
ggplot(data.frame(data_histo = data_histo), aes(x = data_histo)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 2, fill = "lightblue3", col = 'black') +
  ggtitle("Histogramme des observations initiales effectivement observées") +
  
  #densité réelle
  geom_density(aes(color = "Densité réelle du tirage de Y initial"), linewidth = 1) +
  
  #densité théorique
  stat_function(fun = Y_densite, args = list(pars = pars_X0_1d), aes(color = "Densité théorique du tirage de Y initial"), 
                linewidth = 1) +
  
  #création de la légende
  scale_color_manual(name = "Légende", values = c("Densité théorique du tirage de Y initial" = "red", 
                                                  "Densité réelle du tirage de Y initial" = "blue")) +
  
  labs(x = "Valeur des observations Y initiales", y = "Densité du comptage des observations Y initiales")


```

Nous voyons que les deux courbes se recouvrent. Nous avons bien la bonne distribution de Y~0~. Nous représentons les données en deux dimensions avec un histogramme 3D et une heatmap.

```{r représentation des données pour des dimensions mulitples}
#| echo: false

#Séparation des données en intervalles
x_c <- cut(data_2d$y[,1], 70)
y_c <- cut(data_2d$y[,2], 70)

#Jonction des données
z <- table(x_c, y_c)

#Représentation de l'histogramme 3D
hist3D(z=z, border="black", main = "Histogramme 3D des données initiales effectivement observées", xlab = "Valeur dim 1 de Y initial", ylab = "Valeur dim 2 de Y initial", zlab = "Comptage")

#Représentation de la heatmap
image2D(z=z, border="black",main = "Heatmap des données initiales effectivement observées", xlab = "Valeur dim 1 de Y initial", ylab = "Valeur dim 2 de Y initial")

```

Enfin, nous repésentons le déplacement des X et Y par un graphe des points en deux dimensions.

```{r représentation des points en dimension 2}
#| echo: false

df_x <- tibble(x = data_2d$x[,1], y = data_2d$x[,2])
df_y <- tibble(x = data_2d$y[,1], y = data_2d$y[,2])

df_arrows_x <- tibble(
  x_start = df_x$x[-nrow(df_x)],
  y_start = df_x$y[-nrow(df_x)],
  x_end   = df_x$x[-1],
  y_end   = df_x$y[-1]
)

df_arrows_y <- tibble(
  x_start = df_y$x[-nrow(df_y)],
  y_start = df_y$y[-nrow(df_y)],
  x_end   = df_y$x[-1],
  y_end   = df_y$y[-1]
)

df_points <- bind_rows(
  mutate(df_x, Legende = "Points de X"),
  mutate(df_y, Legende = "Points de Y")
)

ggplot() +
  geom_segment(data = df_arrows_y,
               aes(x = x_start, y = y_start, xend = x_end, yend = y_end),
               color = "lightblue4", show.legend = FALSE) +
    geom_segment(data = df_arrows_x,
               aes(x = x_start, y = y_start, xend = x_end, yend = y_end),
               color = "pink3", show.legend = FALSE) +
  geom_point(data = df_points,
             aes(x = x, y = y, color = Legende),
             size = 0.4) +
  scale_color_manual(values = c("Points de X" = "red", "Points de Y" = "blue")) +
  labs(x = "Coordonnée abscisse", y = "Coordonnée ordonnée", title = "Trajectoire des points Y₁ → Y₂ → ... → Y₂₀₀₀ et X₁ → X₂ → ... → X₂₀₀₀") 

```

Il est possible de représenter les données en deux dimensions sur ce graphe mais leur distribution gaussienne rend un résultat brouillon qui empêche une bonne lecture. Nous pouvons cependant voir qu'il y à une convergence des deux séries vers l'origine du repère.

## Création de l'algorithme du filtre de Kalman

Nous implémentons l'algorithme du filtre de Kalman avec la fonction *get_filtering_kalman*.

```{r création de l algorithme du filtre de Kalman}

get_filtering_kalman <- function(pars_X0, pars, data){
  
  #initialisation du tableau de sortie
  n <- nrow(data)
  
  mu_hat = matrix(nrow = n, ncol = pars$dim_x)
  V_hat = P_hat = array(dim = c(pars$dim_x, pars$dim_x, n))
  
  #initialisation de l'algorithme pour le premier terme
  K <- pars_X0$S_x %*% t(pars$A) %*% solve(pars$A %*% pars_X0$S_x %*% t(pars$A) + pars$S_y)
  mu <- pars_X0$m_x + K %*% (data$y[1,] - pars$A %*% pars_X0$m_x)
  V <- (diag(pars$dim_x) - K %*% pars$A) %*% pars_X0$S_x
  Pred_kalm <- pars$matF %*% V %*% t(pars$matF) + pars$S_x
  
  mu_hat[1,] <- as.numeric(mu)
  V_hat[,,1] <- V
  P_hat[,,1] <- Pred_kalm
  
  #boucle de l'algorithme
  for (i in 2:n){
    K <- Pred_kalm %*% t(pars$A) %*% solve(pars$A %*% Pred_kalm %*% t(pars$A) + pars$S_y)
    mu <- pars$matF %*% mu + K %*% (data$y[i,] - pars$A %*% pars$matF %*% mu)
    V <- (diag(pars$dim_x) - K %*% pars$A) %*% Pred_kalm
    Pred_kalm <- pars$matF %*% V %*% t(pars$matF) + pars$S_x
    
    mu_hat[i,] <- as.numeric(mu)
    V_hat[,,i] <- V
    P_hat[,,i] <- Pred_kalm
    
  }
  
  colnames(mu_hat) <- paste0("M", 1:pars$dim_x)
  means <- as.data.frame(mu_hat)
  
  marginal_variances <- matrix(NA_real_, nrow = n, ncol = pars$dim_x)
  for (i in 1:n) {
    marginal_variances[i, ] <- diag(matrix(V_hat[,,i], nrow = pars$dim_x))
  }
  
  
  colnames(marginal_variances) <- paste0("V", 1:pars$dim_x)
  marginal_variances <- as.data.frame(marginal_variances)
  
  formatted_data <- bind_cols(means, marginal_variances) %>%
    mutate(index = 1:nrow(.)) %>%
    pivot_longer(
      cols = -index,
      names_to = c(".value", "dimension"),
      names_pattern = "([MV])(\\d+)"
    ) %>%
    rename(mean = M, Var = V) %>%
    mutate(lower = mean - 1.96 * sqrt(Var),
           upper = mean + 1.96 * sqrt(Var))
  
  return(list(mu_hat = mu_hat, V_hat = V_hat, result_df = formatted_data, Pred_kalm = P_hat, index = 1:n))
}

```

Nous pouvons récupérer les estimations de l'algorithme.

```{r utilisation de l algorithme de filtre}

filter_1d <- get_filtering_kalman(pars_X0_1d, pars_1d, data_1d)
filter_2d <- get_filtering_kalman(pars_X0_2d, pars_2d, data_2d)

```

```{r fonction de création de graphe}
#| echo: false

plot_dimension <- function(result_df, data, filter_result_df = NULL, dim_to_plot = 1) {
  #Filtrer sur la dimension choisie
  df_dim <- result_df %>% filter(dimension == as.character(dim_to_plot))
  
  #Extraction l'état réel et l'observation correspondants
  true_state <- data.frame(index = 1:nrow(data$x), 
                           value = data$x[, dim_to_plot], 
                           type = paste0("x (état réel ", dim_to_plot, ")"))
  
  observations <- data.frame(index = 1:nrow(data$y), 
                             value = data$y[, dim_to_plot], 
                             type = paste0("y (observation ", dim_to_plot, ")"))
  
  #Récupération du filtrage
  filter_df <- NULL
  if (!is.null(filter_result_df)) {
    filter_df <- filter_result_df %>% filter(dimension == as.character(dim_to_plot)) %>%
      mutate(type = paste0("mu_hat (filtrage ", dim_to_plot, ")")) %>%
      select(index, mean, type)
  }
  
  #Récupération du lissage
  smoother_df <- df_dim %>% 
    select(index, mean, lower, upper) %>%
    mutate(type = paste0("mu_hat (lissage ", dim_to_plot, ")"))
  
  #Données pour ribbon IC
  ribbon_df <- df_dim %>% 
    select(index, lower, upper) %>%
    mutate(type = "Intervalle de confiance")
  
  #Préparation des données pour les points
  points_df <- bind_rows(
    true_state,
    observations,
    if (!is.null(filter_df)) filter_df %>% rename(value = mean),
    smoother_df %>% rename(value = mean)
  ) %>%
    mutate(type = factor(type, levels = c(
      paste0("x (état réel ", dim_to_plot, ")"),
      paste0("y (observation ", dim_to_plot, ")"),
      if (!is.null(filter_df)) paste0("mu_hat (filtrage ", dim_to_plot, ")"),
      paste0("mu_hat (lissage ", dim_to_plot, ")")
    )))
  
  
  # Plot
  colors <- c("black", "blue", "red", "darkgreen", "cyan4")
names(colors) <- c(
  paste0("x (état réel ", dim_to_plot, ")"),
  paste0("y (observation ", dim_to_plot, ")"),
  paste0("mu_hat (filtrage ", dim_to_plot, ")"),
  paste0("mu_hat (lissage ", dim_to_plot, ")"),
  "Intervalle de confiance"
)

shapes <- c(16, 4, 1, 1, NA)
names(shapes) <- c(
  paste0("x (état réel ", dim_to_plot, ")"),
  paste0("y (observation ", dim_to_plot, ")"),
  paste0("mu_hat (filtrage ", dim_to_plot, ")"),
  paste0("mu_hat (lissage ", dim_to_plot, ")"),
  "Intervalle de confiance"
)

fills <- c("cyan4")
names(fills) <- c("Intervalle de confiance")

plot <- ggplot() +
  geom_ribbon(data = ribbon_df, aes(x = index, ymin = lower, ymax = upper, fill = type), alpha = 0.2) +
  geom_line(data = smoother_df, aes(x = index, y = mean, color = type), linewidth = 1, alpha = 0.6) +
  geom_point(data = points_df, aes(x = index, y = value, color = type, shape = type), size = 2, na.rm = TRUE) +
  scale_color_manual(name = "Légende", values = colors) +
  scale_fill_manual(name = "Légende", values = fills) +
  scale_shape_manual(name = "Légende", values = shapes) +
  labs(x = "Numéro de l'observation", y = "Valeur",
       title = paste0("Estimation et lissage de la dim ", dim_to_plot, " de x avec intervalle de confiance à 95%"))

if (!is.null(filter_result_df)) {
  plot <- plot + geom_line(data = filter_df, aes(x = index, y = mean, color = type), linewidth = 1)
}
print(plot)
}

```

Puis nous représentons les estimations pour chaque points X avec un intervalle de confiance à 95%.

```{r représentation de l estimation du filtre de Kalman en dim1}
#| echo: false

plot_dimension(filter_1d$result_df, data_1d)

```

L'estimation suit bien les valeurs de la variable estimée. On observe aussi que l'intervalle de confiance n'apparait quasiment pas; c'est sûrement dû a la variance très faible rendue par l'algorithme ce qui implique que l'intervalle prend des valeurs non significatives par rapport à celle des points obtenus.

```{r représentation de l estimation du filtre de Kalman de la dimension 1 en dim2}
#| echo: false

plot_dimension(filter_2d$result_df, data_2d)

```

```{r représentation de l estimation du filtre de Kalman de la dimension 2 en dim2}
#| echo: false

plot_dimension(filter_2d$result_df, data_2d, dim_to_plot = 2)

```

Encore une fois l'estimation semble très bonne pour les deux dimensions et l'intervalle de confiance n'apparaît quasiment pas.

## Création de l'algorithme du lisseur RTS

Nous implémentons maintenant l'algorithme du lisseur RTS pour nos données.

```{r création de l algorithme du lisseur RTS}

get_smoothering_RTS <- function(ini,pars){
  
  #initialisation du tableau de sortie
  n <- nrow(ini$mu_hat)
  mu_hat = matrix(nrow = n, ncol = pars$dim_x)
  V_hat = array(dim = c(pars$dim_x, pars$dim_x, n))
  
  mu_hat[n,] <- t(ini$mu_hat[n,])
  V_hat[,,n] <- ini$V_hat[,,n]
  
  #boucle de l'algorithme
  for (i in (n-1):1){

    C <- ini$V_hat[,,i] %*% t(pars$matF) %*% solve(ini$Pred_kalm[,,i])
    mu_hat_loop <- ini$mu_hat[i,] + C %*% (mu_hat[i+1,] - pars$matF %*% ini$mu_hat[i,])
    V_hat_loop <- ini$V_hat[,,i] + C %*% (V_hat[,,i+1] - ini$Pred_kalm[,,i]) %*% t(C)
    
    mu_hat[i,] <- as.matrix(mu_hat_loop)
    V_hat[,,i] <- V_hat_loop

  }
marginal_variances <- matrix(NA_real_, nrow = n, ncol = pars$dim_x)
  for (i in 1:n) {
    marginal_variances[i, ] <- diag(matrix(V_hat[,,i], nrow = pars$dim_x))
  }
  
  colnames(mu_hat) <- paste0("M", 1:pars$dim_x)
  colnames(marginal_variances) <- paste0("V", 1:pars$dim_x)
  
  means <- as.data.frame(mu_hat)
  marginal_variances <- as.data.frame(marginal_variances)
  
  formatted_data <- bind_cols(means, marginal_variances)%>% 
    mutate(index = 1:nrow(.))%>% 
    pivot_longer(
      cols = -index,
      names_to = c(".value", "dimension"),
      names_pattern = "([MV])(\\d+)"
    ) %>%
    rename(mean = M, Var = V) %>%
    mutate(lower = mean - 1.96 * sqrt(Var),
           upper = mean + 1.96 * sqrt(Var))
  return(list(mu_hat = mu_hat, V_hat = V_hat, result_df = formatted_data))
}

```

Nous pouvons récuperer les estimations.

```{r utilisation de l algorithme de lissage}

smoother_1d <- get_smoothering_RTS(filter_1d, pars_1d)
smoother_2d <- get_smoothering_RTS(filter_2d, pars_2d)

```

Afin de mieux comprendre ces algorithmes nous les mettons en concurrence sur ce graphe.

```{r représentation de l estimation du filtre et du lisseur de Kalman en dim1}
#| echo: false

plot_dimension(smoother_1d$result_df,data_1d,filter_1d$result_df)

```

Les deux estimations sont confondues il est difficile de les distinguer mais im semble qu'elles suivent toutes deux la série des observations réélles.

Comme le graphe précédent, l'estimateur semble bon et l'intervalle de confiance n'apparait pas pour les mêmes raisons.

```{r calcul de la distance entre les deux estimations}

estimation_variation <- abs(smoother_1d$mu_hat - filter_1d$mu_hat)
print(mean(estimation_variation))

```

Nous voyons de plus qu'il n'apparait qu'une courbe; ceci est dû au fait que les deux estimateurs donnent les mêmes valeurs dans ce cas comme nous l'indique la distance moyenne entre les deux estimateurs de $\mu$ ci-dessus.

```{r représentation de l estimation du filtre et du lisseur de Kalman de la dimension 1 en dim2}
#| echo: false
 
plot_dimension(smoother_2d$result_df,data_2d,filter_2d$result_df)

```

Pour la première variable, nous voyons que les deux estimations sont quasiment confondues ce qui nous conforte dans la qualité de celles-ci.

```{r représentation de l estimation du filtre et du lisseur de Kalman de la dimension 2 en dim2}
#| echo: false

plot_dimension(smoother_2d$result_df,data_2d,filter_2d$result_df,2)

```

Mêmes commantaires pour la deuxième dimension.

Nous allons représenter les éllipses de confiance de deux points arbitraires (point n°16 et n°500) pour cela nous avons besoin de définir la *distance de Mahalanobis* donnée par la formule :

$$
D_M(\mathbf{x}) = \sqrt{(\mathbf{x} - \boldsymbol{\mu})^\top \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})}
$$

Nous étudierons son carré :

$$
D_M^{2}(\mathbf{x}) = (\mathbf{x} - \boldsymbol{\mu})^\top \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})
$$

sous la forme de la fonction ci-dessous.

```{r fonction mahalanobis}

get_mahalanobis <- function(x, y, mu, sigma) {
  vect <- c(x, y) - mu
  return(t(vect) %*% solve(sigma) %*% vect)
}

```

Ci dessous les points des estimations du filtrage et du lissage avec mis en couleur le point n°16 en bleu et le point n°500 en rouge.

Le rectangle bleu clair est le zoom des graphes des ellipses de confiance qui viennent ensuite.

```{r étude des ellipses de confiance}
#| echo: false

q <- qchisq(.95, pars_2d$dim_x)


point_filter_16 <- as.data.frame(t(filter_2d$mu_hat[16, ]))
colnames(point_filter_16) <- c("x", "y")

point_filter_500 <- as.data.frame(t(filter_2d$mu_hat[500, ]))
colnames(point_filter_500) <- c("x", "y")

point_smoother_16 <- as.data.frame(t(smoother_2d$mu_hat[16, ]))
colnames(point_smoother_16) <- c("x", "y")

point_smoother_500 <- as.data.frame(t(smoother_2d$mu_hat[500, ]))
colnames(point_smoother_500) <- c("x", "y")

mu_filter_16 <- as.numeric(filter_2d$mu_hat[16, ])  
sigma_filter_16 <- filter_2d$V_hat[,,16]

mu_filter_500 <- as.numeric(filter_2d$mu_hat[500, ])  
sigma_filter_500 <- filter_2d$V_hat[,,500]

mu_smoother_16 <- as.numeric(smoother_2d$mu_hat[16, ])  
sigma_smoother_16 <- smoother_2d$V_hat[,,16]

mu_smoother_500 <- as.numeric(smoother_2d$mu_hat[500, ])  
sigma_smoother_500 <- smoother_2d$V_hat[,,500]

x_seq_16 <- seq(filter_2d$mu_hat[16,1] - 10, filter_2d$mu_hat[16,1] + 10, by = .1)
y_seq_16 <- seq(filter_2d$mu_hat[16,2] - 10, filter_2d$mu_hat[16,2] + 10, by = .1)

x_seq_500 <- seq(filter_2d$mu_hat[500,1] - 10, filter_2d$mu_hat[500,1] + 10, by = .1)
y_seq_500 <- seq(filter_2d$mu_hat[500,2] - 10, filter_2d$mu_hat[500,2] + 10, by = .1)

grille_points1 <- expand.grid(x = x_seq_16, y = y_seq_16)
grille_points2 <- expand.grid(x = x_seq_500, y = y_seq_500)

dist_maha_filter_16 <- mapply(get_mahalanobis, 
                     x = grille_points1$x, 
                     y = grille_points1$y,
                     MoreArgs = list(mu = mu_filter_16, sigma = sigma_filter_16))

dist_maha_filter_500 <- mapply(get_mahalanobis, 
                     x = grille_points2$x, 
                     y = grille_points2$y, 
                     MoreArgs = list(mu = mu_filter_500, sigma = sigma_filter_500))

dist_maha_smoother_16 <- mapply(get_mahalanobis, 
                     x = grille_points1$x, 
                     y = grille_points1$y,
                     MoreArgs = list(mu = mu_smoother_16, sigma = sigma_smoother_16))

dist_maha_smoother_500 <- mapply(get_mahalanobis, 
                     x = grille_points2$x, 
                     y = grille_points2$y, 
                     MoreArgs = list(mu = mu_smoother_500, sigma = sigma_smoother_500))

area_filter_16 <- grille_points1[dist_maha_filter_16 <= q, ]

area_filter_500 <- grille_points2[dist_maha_filter_500 <= q, ]

area_smoother_16 <- grille_points1[dist_maha_smoother_16 <= q, ]

area_smoother_500 <- grille_points2[dist_maha_smoother_500 <= q, ]


ggplot()+
  geom_point(data = filter_2d$mu_hat,
             aes(x = filter_2d$mu_hat[,1], y = filter_2d$mu_hat[,2]),
             size = 0.4)+
  geom_point(data = point_filter_16,
             aes(x = x, y = y),
             size = 2, col = 'red') +
  geom_point(data = point_filter_500,
             aes(x = x, y = y),
             size = 2, col = 'blue') +
  geom_tile(data = area_filter_16, aes(x = x, y = y), fill = "red", alpha = .4)+
  geom_tile(data = area_filter_500, aes(x = x, y = y), fill = "blue", alpha = .4)+
  geom_tile(data = grille_points1, aes(x = x, y = y), fill = "lightblue", alpha = 0.5)+
  geom_tile(data = grille_points2, aes(x = x, y = y), fill = "lightblue", alpha = 0.5)+
  labs(x = "Dimension 1 du filtrage", y = "Dimension 2 du filtrage")+
  ggtitle("Représentation des points estimés par la méthode du filtrage")



ggplot()+
  geom_point(data = smoother_2d$mu_hat,
             aes(x = smoother_2d$mu_hat[,1], y = smoother_2d$mu_hat[,2]),
             size = 0.4)+
geom_point(data = point_smoother_16,
             aes(x = x, y = y),
             size = 2, col = 'red') +
  geom_point(data = point_smoother_500,
             aes(x = x, y = y),
             size = 2, col = 'blue') +
  geom_tile(data = area_smoother_16, aes(x = x, y = y), fill = "red", alpha = .4)+
  geom_tile(data = area_smoother_500, aes(x = x, y = y), fill = "blue", alpha = .4)+
  geom_tile(data = grille_points1, aes(x = x, y = y), fill = "lightblue", alpha = 0.5)+
  geom_tile(data = grille_points2, aes(x = x, y = y), fill = "lightblue", alpha = 0.5)+
  labs(x = "Dimension 1 du lissage", y = "Dimension 2 du lissage")+
  ggtitle("Représentation des points estimés par la méthode du lissage")
```

```{r ellipse de confiance pour l estimation numéro 16}
#| echo: false

x_min_16 <- point_filter_16$x - 5  
x_max_16 <- point_filter_16$x + 5  
y_min_16 <- point_filter_16$y - 5  
y_max_16 <- point_filter_16$y + 5  

ggplot() +
  geom_point(data = filter_2d$mu_hat,
             aes(x = filter_2d$mu_hat[,1], y = filter_2d$mu_hat[,2]),
             size = 0.4) +
  geom_point(data = point_filter_16,
             aes(x = x, y = y),
             size = 2, col = 'red') +
  geom_tile(data = area_filter_16,
            aes(x = x, y = y),
            fill = "red", alpha = 0.3) +
  geom_tile(data = area_smoother_16,
            aes(x = x, y = y),
            fill = "green3", alpha = 0.3) +
  coord_cartesian(xlim = c(x_min_16, x_max_16), ylim = c(y_min_16, y_max_16))+
  labs(x = "Dimension 1 du filtrage", y = "Dimension 2 du filtrage")+
  ggtitle("Ellipses des confiances des méthodes du lissage et du filtrage pour le point 16")
```

Pour l'estimation du point n°16, nous avons représenté l'ellipse de confiance du filtrage en rouge et celle du lissage en vert. On remarque que les deux surfaces sont de même forme, se chevauchent sur la majorité de leur surface. Il est néanmoins visible que l'aire de l'ellipse verte donc du lissage est moins grande que celle du filtrage, donc de la rouge.

```{r ellipse de confiance pour l estimation numéro 500}
#| echo: false

x_min_500 <- point_filter_500$x - 5  
x_max_500 <- point_filter_500$x + 5  
y_min_500 <- point_filter_500$y - 5  
y_max_500 <- point_filter_500$y + 5  

ggplot() +
  geom_point(data = filter_2d$mu_hat,
             aes(x = filter_2d$mu_hat[,1], y = filter_2d$mu_hat[,2]),
             size = 0.4) +
  geom_point(data = point_filter_500,
             aes(x = x, y = y),
             size = 2, col = 'blue') +
  geom_tile(data = area_filter_500,
            aes(x = x, y = y),
            fill = "blue", alpha = 0.4) +
  geom_tile(data = area_smoother_500,
            aes(x = x, y = y),
            fill = "grey3", alpha = 0.3) +
  coord_cartesian(xlim = c(x_min_500, x_max_500), ylim = c(y_min_500, y_max_500))+
    labs(x = "Dimension 1 du filtrage", y = "Dimension 2 du filtrage")+
  ggtitle("Ellipses des confiances des méthodes du lissage et du filtrage pour le point 500")

```

Pour le point n°500, nous avons représenté l'ellipse de confiance du filtrage en bleu et celle du lissage en gris. La ressemble est d'autant plus forte à l'exception que cette fois-ci l'ellipse de lissage en gris est incluse dans l'ellipse de filtrage bleu.

# Filtrage de Doucet

La méthode du filtre de Kalman se reposait sur le fait que les distributions étudiées étaient des lois normales multivariées linéaires. Ceci permettait une résolution calculatoire qui n'est, dans les faits, que rarement possible pour les autres distributions. L'objectif de l'algorithme du Doucet est de s'attaquer aux cas où la résolution des intégrales n'est pas possible dans le modèle HMM.

Pour cela, cet algorithme utilise la méthode de Monte-Carlo. Dans un premier temps, nous allons réutiliser le contexte de la partie du filtre de Kalman afin de montrer l'équivalence des méthodes dans le cas spécifique des lois normales.

## Contexte du filtre de Kalman

Nous commençons par étudier le cas initial, pour cela nous commençons par créer la fonction *get_MCMC_SNIS* qui donne l'estimateur de Monte-Carlo d'échantillonage préférentiel autonormalisé.

```{r SNIS}

get_MCMC_SNIS <- function(fun, pfun, qfun, sampling_method, m = 1000){
  
  samples <- sampling_method(m)
  W <- pfun(samples)/ qfun(samples)
  W_tilde <- W / sum(W)
  I_hat <- sum(W_tilde * fun(samples))
  
  return(list(W_tilde = W_tilde, X = samples, I_hat = I_hat))
  
}

```

Nous récupérons l'estimateur pour une dimension de X~0~ de la fonction ci-dessus et nous le comparons avec celui du filtrage de Kalman et du lissage RTS.

```{r utilisation du SNIS en dim1 dans le contexte précédent}
#| echo: false

id <- function(x) return(x)
pfun <- function(x) return(dnorm(x,pars_X0_1d$m_x,sqrt(pars_X0_1d$S_x)))
get_samples_X0 <- function(m) return(rnorm(m, mean = pars_X0_1d$m_x, sd = sqrt(pars_X0_1d$S_x)))


MCMC_SNIS_X0 <- get_MCMC_SNIS(id, pfun, id, get_samples_X0)

paste0("Différence absolue pour le terme d'initialisation entre l'estimateur de Monte-Carlo et celui du filtre de Kalman : ", round(abs(MCMC_SNIS_X0$I_hat - filter_1d$mu_hat[1]),4))

paste0("Différence absolue pour le terme d'initialisation entre l'estimateur de Monte-Carlo et celui du lisseur RTS : ", round(abs(MCMC_SNIS_X0$I_hat - smoother_1d$mu_hat[1]),4))
```

La différence absolue étant d'un ordre de grandeur bien inférieur de celui des valeurs nous pouvons être satisfait par la proximité des estimateurs.


```{r tests et stockage de code}
#| echo: false

get_simulation_alt <- function(n_sim, pars){
  x <- mixtools::rmvnorm(n = n_sim, mu = pars$m_x,
                         sigma = as.matrix(pars$S_x))
  y <- t(pars$A %*% t(x) +  pars$b) +
    mixtools::rmvnorm(n = n_sim, mu = rep(0, pars$dim_y),
                      sigma = as.matrix(pars$S_y))
  return(list(x = x, y = y))
}
```
